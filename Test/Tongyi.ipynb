{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-11T03:20:58.932972Z",
     "start_time": "2025-11-11T03:20:57.694640Z"
    }
   },
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "chatLLM = ChatTongyi(\n",
    "    streaming=True,\n",
    "    api_key = \"sk-43070f4cd1074965a93a03d6d5333cd8\",\n",
    "    model=\"qwen3-max\"\n",
    ")\n",
    "res = chatLLM.stream([HumanMessage(content=\"hi\")], streaming=True)\n",
    "for r in res:\n",
    "    print(\"chat resp:\", r.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat resp: Hello\n",
      "chat resp: ! How can\n",
      "chat resp:  I help\n",
      "chat resp:  you today?\n",
      "chat resp:  ðŸ˜Š\n",
      "chat resp: \n",
      "chat resp: \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T06:42:57.873229Z",
     "start_time": "2025-11-11T06:42:56.819741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\n",
    "    model=\"qwen3-max\",\n",
    "    api_key = \"sk-43070f4cd1074965a93a03d6d5333cd8\",\n",
    "    model_provider = \"tongyi\"\n",
    ")"
   ],
   "id": "50ae266e1d1c8a29",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v-lizhengfan\\PythonProjects\\langchainLearn\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported model_provider='tongyi'.\n\nSupported model providers are: perplexity, bedrock_converse, mistralai, azure_ai, cohere, google_genai, ollama, ibm, google_vertexai, google_anthropic_vertex, deepseek, xai, together, groq, azure_openai, anthropic, huggingface, openai, bedrock, fireworks",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_models\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m init_chat_model\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m model = \u001B[43minit_chat_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mqwen3-max\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msk-43070f4cd1074965a93a03d6d5333cd8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_provider\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtongyi\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PythonProjects\\langchainLearn\\.venv\\Lib\\site-packages\\langchain\\chat_models\\base.py:311\u001B[39m, in \u001B[36minit_chat_model\u001B[39m\u001B[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001B[39m\n\u001B[32m    303\u001B[39m     warnings.warn(\n\u001B[32m    304\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_prefix\u001B[38;5;132;01m=}\u001B[39;00m\u001B[33m has been set but no fields are configurable. Set \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    305\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`configurable_fields=(...)` to specify the model params that are \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    306\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mconfigurable.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    307\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    308\u001B[39m     )\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m configurable_fields:\n\u001B[32m--> \u001B[39m\u001B[32m311\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_init_chat_model_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_provider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m model:\n\u001B[32m    317\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m] = model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PythonProjects\\langchainLearn\\.venv\\Lib\\site-packages\\langchain\\chat_models\\base.py:451\u001B[39m, in \u001B[36m_init_chat_model_helper\u001B[39m\u001B[34m(model, model_provider, **kwargs)\u001B[39m\n\u001B[32m    449\u001B[39m supported = \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join(_SUPPORTED_PROVIDERS)\n\u001B[32m    450\u001B[39m msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnsupported \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_provider\u001B[38;5;132;01m=}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mSupported model providers are: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msupported\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m451\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[31mValueError\u001B[39m: Unsupported model_provider='tongyi'.\n\nSupported model providers are: perplexity, bedrock_converse, mistralai, azure_ai, cohere, google_genai, ollama, ibm, google_vertexai, google_anthropic_vertex, deepseek, xai, together, groq, azure_openai, anthropic, huggingface, openai, bedrock, fireworks"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "582b7d3a005d795c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
